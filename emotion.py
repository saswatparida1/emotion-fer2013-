# -*- coding: utf-8 -*-
"""emotion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OFUSgk33oBQNCGln5RtRcsTtuLm4S8id
"""

import pandas as pd
import numpy as np

!pip install neptune-cli
!pip install livelossplot
from livelossplot import PlotLossesKeras

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
file5 = drive.CreateFile({'id': '1Lnd24v-QsPgx6W5iiTCwaOZfEMNqb84L'})
file5.GetContentFile('fer2013.csv')

data=pd.read_csv('fer2013.csv')
data1 = data['pixels']
data1 = [ dat.split() for dat in data1]
data1 = np.array(data1)
data1 = data1.astype('float64')
data1 = [[np.divide(d,255.0) for d in dat] for dat in data1]


data2 = data['emotion']

import pandas as pd
import numpy as np

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten,BatchNormalization
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.utils import np_utils
from keras.optimizers import SGD, RMSprop
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from livelossplot import PlotLossesKeras
from keras import regularizers
from keras.regularizers import l2
from keras.constraints import maxnorm
import keras

np.random.seed(2222)
X_train=np.asarray(data1)
Y_train=np.asarray(data2)
Y_train=np_utils.to_categorical(Y_train)
print(X_train.shape)
print(Y_train.shape)

shapex , shapey = 48, 48
X_train = X_train.reshape(X_train.shape[0] ,  shapex , shapey,1)
X_test=X_train[32000:]
Y_test=Y_train[32000:]
X_train=X_train[:32000]
Y_train=Y_train[:32000]
vx=X_train[30000:]
vy=Y_train[30000:]
X_train=X_train[:30000]
Y_train=Y_train[:30000]
print(vx.shape)
print(vy.shape)
print(X_test.shape)
print(Y_test.shape)
print(X_train.shape)
print(Y_train.shape)

weight_decay=0.001
model=Sequential()
model.add(Conv2D(64, (3, 3), padding='same',input_shape=(48,48,1),kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Dense(7))
model.add(Activation('softmax'))
model.summary()

#opt = keras.optimizers.Adadelta(0.1,decay=1e-4)
ams=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)
model.compile(optimizer = ams, loss = 'categorical_crossentropy', metrics = ['accuracy'])
datagen = ImageDataGenerator(
    rotation_range=15,
    horizontal_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1,
    #zoom_range=0.3
    )
datagen.fit(X_train)

log=model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),
                    steps_per_epoch = len(X_train) / 32, epochs=150, callbacks=[PlotLossesKeras()], validation_data=(vx, vy))
#log=cnn.fit(x_train ,y_train,validation_split=0.2, callbacks=[PlotLossesKeras()], epochs = 50)
plt.plot(log.history['acc'])
plt.plot(log.history['val_acc'])
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.plot(log.history['loss'])
plt.plot(log.history['val_loss'])
plt.title('model error')
plt.ylabel('error')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import h5py
json_string = model.to_json()
model.save_weights('./models/Face_model_weights.h5')
open('./models/Face_model_architecture.json', 'w').write(json_string)




